{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    precision_recall_curve, average_precision_score, roc_curve,fbeta_score\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import shap\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# 设置numpy随机种子\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df = pd.read_parquet('')\n",
    "#数据\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc146ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一部分：数据预处理与特征工程\n",
    "#省略此部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a4fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征工程后，分离最终验证集\n",
    "# ===============================\n",
    "\n",
    "# 获取所有正样本的索引\n",
    "positive_indices = y[y == 1].index.tolist()\n",
    "print(f\"总正样本数量: {len(positive_indices)}\")\n",
    "# 从正样本中随机选择30个作为最终验证集\n",
    "final_validation_indices = np.random.choice(positive_indices, size=30, replace=False)\n",
    "training_positive_indices = [idx for idx in positive_indices if idx not in final_validation_indices]\n",
    "\n",
    "print(f\"最终验证集正样本: 30个\")\n",
    "print(f\"训练用正样本: {len(training_positive_indices)}个\")\n",
    "\n",
    "# 为最终验证集选择合理数量的负样本\n",
    "negative_indices = y[y == 0].index.tolist()\n",
    "# 选择合理的负样本数量：使用原始比例，即116550个负样本\n",
    "final_validation_negative_count = 116550 \n",
    "final_validation_negative_indices = np.random.choice(negative_indices, \n",
    "                                                   size=final_validation_negative_count, \n",
    "                                                   replace=False)\n",
    "\n",
    "print(f\"最终验证集负样本: {final_validation_negative_count}个\")\n",
    "print(f\"最终验证集总样本: {30 + final_validation_negative_count}个\")\n",
    "print(f\"最终验证集正负比例: 1:{final_validation_negative_count//30}\")\n",
    "\n",
    "# 创建最终验证集\n",
    "final_validation_all_indices = list(final_validation_indices) + list(final_validation_negative_indices)\n",
    "X_final_validation = X.loc[final_validation_all_indices]\n",
    "y_final_validation = y.loc[final_validation_all_indices]\n",
    "\n",
    "# 从训练数据中移除最终验证集样本（使用更高效的方法）\n",
    "final_validation_indices_set = set(final_validation_all_indices)\n",
    "remaining_mask = ~X.index.isin(final_validation_all_indices)\n",
    "X_training_pool = X[remaining_mask]\n",
    "y_training_pool = y[remaining_mask]\n",
    "\n",
    "print(f\"数据分布:\")\n",
    "print(f\"训练池 - 正样本: {(y_training_pool == 1).sum()}个\")\n",
    "print(f\"训练池 - 负样本: {(y_training_pool == 0).sum()}个\")\n",
    "print(f\"最终验证集 - 正样本: {(y_final_validation == 1).sum()}个\")\n",
    "print(f\"最终验证集 - 负样本: {(y_final_validation == 0).sum()}个\")\n",
    "print(f\"最终验证集转化率: {y_final_validation.mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d56ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多重欠采样策略：构建多个平衡数据集进行交叉验证（使用119个正样本）\n",
    "# =======================================================\n",
    "\n",
    "\n",
    "print(\"多重欠采样平衡数据集构建策略（保留30个正样本作为最终验证）\")\n",
    "print(\"策略：119个正样本 + 多次随机抽取119个负样本\")\n",
    "print(\"模型：逻辑回归 + 交叉验证\")\n",
    "print(\"最终验证：30个正样本的独立测试集\")\n",
    "\n",
    "def create_multiple_balanced_datasets(X, y, n_datasets=100, positive_sample_multiplier=1):\n",
    "    \"\"\"\n",
    "    创建多个平衡数据集\n",
    "    \n",
    "    参数:\n",
    "    - X: 特征矩阵\n",
    "    - y: 标签\n",
    "    - n_datasets: 要创建的数据集数量\n",
    "    - positive_sample_multiplier: 负样本数量相对于正样本的倍数（1表示1:1平衡）\n",
    "    \"\"\"\n",
    "    \n",
    "    # 分离正负样本\n",
    "    positive_indices = np.where(y == 1)[0]\n",
    "    negative_indices = np.where(y == 0)[0]\n",
    "    \n",
    "    n_positive = len(positive_indices)\n",
    "    n_negative_per_dataset = n_positive * positive_sample_multiplier\n",
    "    \n",
    "    print(f\"📊 数据分布:\")\n",
    "    print(f\"   正样本数量: {n_positive}\")\n",
    "    print(f\"   负样本数量: {len(negative_indices)}\")\n",
    "    print(f\"   每个数据集负样本抽取: {n_negative_per_dataset}\")\n",
    "    print(f\"   将创建 {n_datasets} 个平衡数据集\")\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    # 设置随机种子，确保可重现性\n",
    "    #确保不重复抽样\n",
    "    for i in range(n_datasets):\n",
    "        # 随机抽取负样本\n",
    "        selected_negative_indices = np.random.choice(\n",
    "            negative_indices, \n",
    "            size=n_negative_per_dataset, \n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        # 合并正负样本索引\n",
    "        balanced_indices = np.concatenate([positive_indices, selected_negative_indices])\n",
    "        \n",
    "        # 创建平衡数据集\n",
    "        X_balanced = X.iloc[balanced_indices]\n",
    "        y_balanced = y.iloc[balanced_indices]\n",
    "        \n",
    "        datasets.append({\n",
    "            'X': X_balanced,\n",
    "            'y': y_balanced,\n",
    "            'indices': balanced_indices,\n",
    "            'dataset_id': i + 1\n",
    "        })\n",
    "        \n",
    "        print(f\"   数据集 {i+1}: {len(y_balanced)} 样本 (正:{sum(y_balanced)}, 负:{len(y_balanced)-sum(y_balanced)})\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tabulate import tabulate  # 如未安装：pip install tabulate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 执行多重欠采样策略（使用训练池数据）\n",
    "print(\"开始执行多重欠采样策略...\")\n",
    "\n",
    "# 创建多个平衡数据集（使用剩余的119个正样本）\n",
    "balanced_datasets = create_multiple_balanced_datasets(\n",
    "    X_training_pool, y_training_pool, \n",
    "    n_datasets=10,  # 创建100个数据集\n",
    "    positive_sample_multiplier=1  # 1:1 平衡\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_logistic_regression_on_datasets(datasets, feature_names):\n",
    "    \"\"\"\n",
    "    在多个数据集上训练逻辑回归模型并进行交叉验证\n",
    "    包含L2正则化、概率校准和SHAP分析\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n🤖 开始训练逻辑回归模型...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 存储所有结果\n",
    "    all_results = []\n",
    "    shap_values_all = []\n",
    "    \n",
    "    # L2正则化参数网格\n",
    "    C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        print(f\"\\n--- 训练数据集 {dataset['dataset_id']} ---\")\n",
    "        \n",
    "        X_train = dataset['X']\n",
    "        y_train = dataset['y']\n",
    "        \n",
    "        # 标准化特征\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # 1. L2正则化参数调优 - 以F2分数为优化目标\n",
    "        print(\"🔍 进行L2正则化参数调优（优化F2分数）...\")\n",
    "        param_grid = {\n",
    "            'C': C_values,\n",
    "            'l1_ratio': [0],  # 纯L2正则化\n",
    "            'penalty': ['elasticnet'],\n",
    "            'solver': ['saga'],\n",
    "            'max_iter': [1000],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "        \n",
    "        # 定义F2分数评估器\n",
    "        from sklearn.metrics import fbeta_score, make_scorer\n",
    "        f2_scorer = make_scorer(fbeta_score, beta=2, zero_division=0)\n",
    "        \n",
    "        base_model = LogisticRegression(random_state=42)\n",
    "        grid_search = GridSearchCV(\n",
    "            base_model, \n",
    "            param_grid, \n",
    "            cv=5, \n",
    "            scoring=f2_scorer,  # 使用F2分数作为优化目标\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_C = grid_search.best_params_['C']\n",
    "        best_f2_score = grid_search.best_score_\n",
    "        print(f\"最佳L2正则化参数 C = {best_C}\")\n",
    "        print(f\"最佳F2分数 = {best_f2_score:.4f}\")\n",
    "        \n",
    "        # 2. 交叉验证评估（重点关注F2分数）\n",
    "        cv_scores = {\n",
    "            'accuracy': cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='accuracy'),\n",
    "            'precision': cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='precision'),\n",
    "            'recall': cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='recall'),\n",
    "            'f1': cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='f1'),\n",
    "            'f2': cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring=f2_scorer),\n",
    "            'roc_auc': cross_val_score(best_model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "        }\n",
    "        \n",
    "        # 3. 训练最终模型\n",
    "        best_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # 4. 概率校准\n",
    "        print(\"📊 进行概率校准...\")\n",
    "        # 使用Platt scaling进行概率校准\n",
    "        calibrated_model = CalibratedClassifierCV(best_model, method='sigmoid', cv=3)\n",
    "        calibrated_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # 5. SHAP分析\n",
    "        print(\"🔬 进行SHAP分析...\")\n",
    "        try:\n",
    "            # 为线性模型创建SHAP explainer\n",
    "            explainer = shap.LinearExplainer(best_model, X_train_scaled)\n",
    "            shap_values = explainer.shap_values(X_train_scaled)\n",
    "            \n",
    "            # 计算特征重要性\n",
    "            feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "            \n",
    "            shap_result = {\n",
    "                'explainer': explainer,\n",
    "                'shap_values': shap_values,\n",
    "                'feature_importance': feature_importance,\n",
    "                'feature_names': feature_names\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"SHAP分析出错: {e}\")\n",
    "            shap_result = None\n",
    "        \n",
    "        # 6. 存储结果\n",
    "        result = {\n",
    "            'dataset_id': dataset['dataset_id'],\n",
    "            'model': best_model,\n",
    "            'calibrated_model': calibrated_model,\n",
    "            'scaler': scaler,\n",
    "            'best_C': best_C,\n",
    "            'cv_scores': cv_scores,\n",
    "            'mean_scores': {metric: scores.mean() for metric, scores in cv_scores.items()},\n",
    "            'std_scores': {metric: scores.std() for metric, scores in cv_scores.items()},\n",
    "            'shap_result': shap_result\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "        \n",
    "        # 打印交叉验证结果\n",
    "        print(f\"交叉验证结果 (5折):\")\n",
    "        for metric, scores in cv_scores.items():\n",
    "            print(f\"  {metric:10}: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def plot_calibration_curves(y_true, y_proba_orig, y_proba_calib):\n",
    "    \"\"\"绘制概率校准曲线\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 子图1：可靠性图\n",
    "    plt.subplot(1, 3, 1)\n",
    "    fraction_of_positives_orig, mean_predicted_value_orig = calibration_curve(y_true, y_proba_orig, n_bins=10)\n",
    "    fraction_of_positives_calib, mean_predicted_value_calib = calibration_curve(y_true, y_proba_calib, n_bins=10)\n",
    "    \n",
    "    plt.plot(mean_predicted_value_orig, fraction_of_positives_orig, \"s-\", label=\"原始模型\", color='red')\n",
    "    plt.plot(mean_predicted_value_calib, fraction_of_positives_calib, \"o-\", label=\"校准后模型\", color='blue')\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"完美校准\")\n",
    "    plt.xlabel(\"平均预测概率\")\n",
    "    plt.ylabel(\"实际正例比例\")\n",
    "    plt.title(\"可靠性图（校准曲线）\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 子图2：概率分布\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(y_proba_orig, bins=20, alpha=0.7, label='原始预测概率', color='red', density=True)\n",
    "    plt.hist(y_proba_calib, bins=20, alpha=0.7, label='校准后预测概率', color='blue', density=True)\n",
    "    plt.xlabel('预测概率')\n",
    "    plt.ylabel('密度')\n",
    "    plt.title('预测概率分布')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 子图3：ROC曲线\n",
    "    plt.subplot(1, 3, 3)\n",
    "    fpr_orig, tpr_orig, _ = roc_curve(y_true, y_proba_orig)\n",
    "    fpr_calib, tpr_calib, _ = roc_curve(y_true, y_proba_calib)\n",
    "    \n",
    "    plt.plot(fpr_orig, tpr_orig, label=f'原始模型 (AUC = {roc_auc_score(y_true, y_proba_orig):.3f})', color='red')\n",
    "    plt.plot(fpr_calib, tpr_calib, label=f'校准后模型 (AUC = {roc_auc_score(y_true, y_proba_calib):.3f})', color='blue')\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='随机模型')\n",
    "    plt.xlabel('假正例率')\n",
    "    plt.ylabel('真正例率')\n",
    "    plt.title('ROC曲线')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f0d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练多个逻辑回归模型\n",
    "training_results = train_logistic_regression_on_datasets(balanced_datasets, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0b347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在最终验证集上测试模型性能\n",
    "# =====================================\n",
    "\n",
    "def evaluate_on_final_validation(training_results, X_final_val, y_final_val):\n",
    "    \"\"\"\n",
    "    在最终验证集（30个正样本）上评估集成模型性能\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"最终验证集评估（真正的未见过数据）\")\n",
    "    \n",
    "    # 收集所有模型的预测概率\n",
    "    all_predictions = []\n",
    "    all_predictions_binary = []\n",
    "    \n",
    "    for result in training_results:\n",
    "        model = result['model']\n",
    "        scaler = result['scaler']\n",
    "        \n",
    "        # 标准化验证集\n",
    "        X_val_scaled = scaler.transform(X_final_val)\n",
    "        \n",
    "        # 预测概率\n",
    "        y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        y_pred = model.predict(X_val_scaled)\n",
    "        \n",
    "        all_predictions.append(y_pred_proba)\n",
    "        all_predictions_binary.append(y_pred)\n",
    "    \n",
    "    # 集成预测（平均概率）\n",
    "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
    "    ensemble_pred = (ensemble_proba >= 0.5).astype(int)\n",
    "    # 不同阈值下的性能\n",
    "    print(f\"最终验证集基本信息:\")\n",
    "    print(f\"   总样本数: {len(y_final_val)}\")\n",
    "    print(f\"   正样本数: {y_final_val.sum()}\")\n",
    "    print(f\"   负样本数: {len(y_final_val) - y_final_val.sum()}\")\n",
    "    print(f\"   转化率: {y_final_val.mean():.6f}\")\n",
    "    \n",
    "    print(f\"\\n📈 最终验证集在不同阈值下的性能:\")\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    final_threshold_results = []\n",
    "    \n",
    "    print(\"阈值    精确率   召回率    F1分数   识别转化用户数  预测正样本数\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_at_threshold = (ensemble_proba >= threshold).astype(int)\n",
    "        \n",
    "        precision = precision_score(y_final_val, pred_at_threshold, zero_division=0)\n",
    "        recall = recall_score(y_final_val, pred_at_threshold, zero_division=0)\n",
    "        f1 = f1_score(y_final_val, pred_at_threshold, zero_division=0)\n",
    "        identified_converted = int(recall * y_final_val.sum())\n",
    "        predicted_positive = pred_at_threshold.sum()\n",
    "        \n",
    "        final_threshold_results.append({\n",
    "            'threshold': threshold,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'identified_converted': identified_converted,\n",
    "            'predicted_positive': predicted_positive\n",
    "        })\n",
    "        \n",
    "        print(f\"{threshold:.1f}     {precision:.4f}   {recall:.4f}    {f1:.4f}        {identified_converted}/{y_final_val.sum()}           {predicted_positive}\")\n",
    "    \n",
    "    # 找到最佳性能\n",
    "    best_recall_result = max(final_threshold_results, key=lambda x: x['recall'])\n",
    "    best_f1_result = max(final_threshold_results, key=lambda x: x['f1'])\n",
    "    \n",
    "    print(f\"最终验证集最佳性能:\")\n",
    "    print(f\"最高召回率: {best_recall_result['recall']:.4f} (阈值: {best_recall_result['threshold']:.1f})\")\n",
    "    print(f\"   识别转化用户: {best_recall_result['identified_converted']}/{y_final_val.sum()}\")\n",
    "    print(f\"   对应精确率: {best_recall_result['precision']:.4f}\")\n",
    "    print(f\"   预测正样本数: {best_recall_result['predicted_positive']}\")\n",
    "    \n",
    "    print(f\"\\n最高F1分数: {best_f1_result['f1']:.4f} (阈值: {best_f1_result['threshold']:.1f})\")\n",
    "    print(f\"   召回率: {best_f1_result['recall']:.4f}\")\n",
    "    print(f\"   精确率: {best_f1_result['precision']:.4f}\")\n",
    "    print(f\"   识别转化用户: {best_f1_result['identified_converted']}/{y_final_val.sum()}\")\n",
    "    print(f\"   预测正样本数: {best_f1_result['predicted_positive']}\")\n",
    "    \n",
    "    # 模型置信度分析\n",
    "    print(f\"预测置信度分析:\")\n",
    "    positive_probas = ensemble_proba[y_final_val == 1]  # 真正转化用户的预测概率\n",
    "    negative_probas = ensemble_proba[y_final_val == 0]  # 真正未转化用户的预测概率\n",
    "    \n",
    "    print(f\"真正转化用户的平均预测概率: {positive_probas.mean():.4f}\")\n",
    "    print(f\"真正转化用户的预测概率范围: [{positive_probas.min():.4f}, {positive_probas.max():.4f}]\")\n",
    "    print(f\"真正未转化用户的平均预测概率: {negative_probas.mean():.4f}\")\n",
    "    print(f\"真正未转化用户的预测概率范围: [{negative_probas.min():.4f}, {negative_probas.max():.4f}]\")\n",
    "    \n",
    "    # 绘制概率分布\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(positive_probas, bins=20, alpha=0.7, label='转化用户', color='red', density=True)\n",
    "    plt.hist(negative_probas, bins=20, alpha=0.7, label='未转化用户', color='blue', density=True)\n",
    "    plt.xlabel('预测概率')\n",
    "    plt.ylabel('密度')\n",
    "    plt.title('最终验证集：预测概率分布')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    # 混淆矩阵（使用最佳F1阈值）\n",
    "    best_pred = (ensemble_proba >= best_f1_result['threshold']).astype(int)\n",
    "    cm = confusion_matrix(y_final_val, best_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'最终验证集混淆矩阵\\\\n(阈值={best_f1_result[\"threshold\"]:.1f})')\n",
    "    plt.xlabel('预测值')\n",
    "    plt.ylabel('实际值')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. SHAP分析\n",
    "    print(\"🔬 进行SHAP分析...\")\n",
    "    try:\n",
    "        # 为线性模型创建SHAP explainer\n",
    "        explainer = shap.LinearExplainer(result['model'], X_final_val)\n",
    "        shap_values = explainer.shap_values(X_final_val)\n",
    "        \n",
    "        # 计算特征重要性\n",
    "        feature_importance = np.abs(shap_values).mean(axis=0)\n",
    "        \n",
    "        shap_result = {\n",
    "            'explainer': explainer,\n",
    "            'shap_values': shap_values,\n",
    "            'feature_importance': feature_importance,\n",
    "            'feature_names': feature_names\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP分析出错: {e}\")\n",
    "        shap_result = None\n",
    "    print(shap_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'ensemble_proba': ensemble_proba,\n",
    "        'ensemble_pred': ensemble_pred,\n",
    "        'threshold_results': final_threshold_results,\n",
    "        'best_recall': best_recall_result,\n",
    "        'best_f1': best_f1_result,\n",
    "        'positive_probas': positive_probas,\n",
    "        'negative_probas': negative_probas,\n",
    "        'shap_result': shap_result\n",
    "    }\n",
    "\n",
    "# 执行最终验证集评估\n",
    "print(\"开始最终验证集评估...\")\n",
    "final_validation_results = evaluate_on_final_validation(\n",
    "    training_results, \n",
    "    X_final_validation, \n",
    "    y_final_validation\n",
    ")\n",
    "ensemble_results = final_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14acbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full_test = X_final_validation\n",
    "y_full_test = y_final_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义缺失的函数和生成策略信息\n",
    "# =====================================\n",
    "\n",
    "def generate_threshold_strategies(ensemble_results):\n",
    "    \"\"\"\n",
    "    从集成结果中生成不同阈值策略\n",
    "    \"\"\"\n",
    "    \n",
    "    threshold_results = ensemble_results['threshold_results']\n",
    "    \n",
    "    # 找到最佳策略\n",
    "    best_recall = max(threshold_results, key=lambda x: x['recall'])\n",
    "    best_f1 = max(threshold_results, key=lambda x: x['f1'])\n",
    "    best_precision = max(threshold_results, key=lambda x: x['precision'])\n",
    "    \n",
    "    # 找到业务平衡点（F1在0.3以上且召回率较高）\n",
    "    balanced_candidates = [r for r in threshold_results if r['f1'] >= 0.3]\n",
    "    business_balanced = max(balanced_candidates, key=lambda x: x['recall']) if balanced_candidates else best_f1\n",
    "    \n",
    "    strategies = [\n",
    "        ('最高召回率策略', best_recall),\n",
    "        ('最高F1策略', best_f1), \n",
    "        ('最高精确率策略', best_precision),\n",
    "        ('业务平衡策略', business_balanced)\n",
    "    ]\n",
    "    \n",
    "    print(\"🎯 生成的阈值策略:\")\n",
    "    print(\"=\"*60)\n",
    "    for name, strategy in strategies:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  阈值: {strategy['threshold']:.1f}\")\n",
    "        print(f\"  召回率: {strategy['recall']:.4f}\")\n",
    "        print(f\"  精确率: {strategy['precision']:.4f}\")\n",
    "        print(f\"  F1分数: {strategy['f1']:.4f}\")\n",
    "        print(f\"  识别转化用户: {strategy['identified_converted']}\")\n",
    "        print(\"\")\n",
    "    \n",
    "    return strategies\n",
    "\n",
    "def display_final_confusion_matrices(ensemble_results, y_test):\n",
    "    \"\"\"\n",
    "    显示关键阈值的混淆矩阵\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📊 显示关键阈值的混淆矩阵\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 生成策略\n",
    "    strategies = generate_threshold_strategies(ensemble_results)\n",
    "    \n",
    "    # 选择几个关键策略显示混淆矩阵\n",
    "    key_strategies = [strategies[0], strategies[1]]  # 最高召回率和最高F1\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(key_strategies), figsize=(6*len(key_strategies), 5))\n",
    "    if len(key_strategies) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (name, strategy) in enumerate(key_strategies):\n",
    "        threshold = strategy['threshold']\n",
    "        predictions = (ensemble_results['ensemble_proba'] >= threshold).astype(int)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, predictions)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "        axes[i].set_title(f'{name}\\n阈值={threshold:.1f}')\n",
    "        axes[i].set_xlabel('预测值')\n",
    "        axes[i].set_ylabel('实际值')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return strategies\n",
    "\n",
    "# 生成策略信息\n",
    "print(\"🔧 修复缺失的变量...\")\n",
    "final_strategies = generate_threshold_strategies(ensemble_results)\n",
    "\n",
    "print(\"\\n✅ 策略信息生成完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义集成预测器类 (移到模块级别以支持pickle序列化)\n",
    "# =========================================================\n",
    "\n",
    "class EnsemblePredictor:\n",
    "    \"\"\"\n",
    "    集成预测器类 - 用于加载和使用训练好的多个逻辑回归模型\n",
    "    \"\"\"\n",
    "    def __init__(self, models, feature_names, threshold_strategies):\n",
    "        self.models = models  # 包含model和scaler的列表\n",
    "        self.feature_names = feature_names\n",
    "        self.threshold_strategies = threshold_strategies\n",
    "        self.n_models = len(models)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率（集成平均）\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X[self.feature_names].values\n",
    "        \n",
    "        all_probas = []\n",
    "        for model_info in self.models:\n",
    "            X_scaled = model_info['scaler'].transform(X)\n",
    "            proba = model_info['model'].predict_proba(X_scaled)[:, 1]\n",
    "            all_probas.append(proba)\n",
    "        \n",
    "        return np.mean(all_probas, axis=0)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas >= threshold).astype(int)\n",
    "    \n",
    "    def predict_with_strategy(self, X, strategy_name):\n",
    "        \"\"\"使用特定策略预测\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        \n",
    "        strategy_thresholds = {\n",
    "            '最高召回率策略': self.threshold_strategies[0][1]['threshold'],\n",
    "            '最高F1策略': self.threshold_strategies[1][1]['threshold'],\n",
    "            '最高精确率策略': self.threshold_strategies[2][1]['threshold'],\n",
    "            '业务平衡策略': self.threshold_strategies[3][1]['threshold']\n",
    "        }\n",
    "        \n",
    "        threshold = strategy_thresholds.get(strategy_name, 0.5)\n",
    "        return (probas >= threshold).astype(int), probas\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"获取特征名称\"\"\"\n",
    "        return self.feature_names\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"获取模型信息\"\"\"\n",
    "        return {\n",
    "            'n_models': self.n_models,\n",
    "            'feature_names': self.feature_names,\n",
    "            'available_strategies': ['最高召回率策略', '最高F1策略', '最高精确率策略', '业务平衡策略']\n",
    "        }\n",
    "\n",
    "print(\"✅ EnsemblePredictor 类定义完成，支持pickle序列化\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7124104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修复版：保存模型（使用全局EnsemblePredictor类）\n",
    "# =============================================\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "def save_ensemble_model_fixed(training_results, ensemble_results, feature_names, strategies):\n",
    "    \"\"\"\n",
    "    保存训练好的集成模型和相关信息（修复pickle错误）\n",
    "    \"\"\"\n",
    "    print(f\"\\n💾 保存模型...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 创建保存目录\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = f\"./saved_models_{timestamp}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. 保存所有单个模型\n",
    "    print(\"保存10个单独的逻辑回归模型...\")\n",
    "    individual_models = {}\n",
    "    for result in training_results:\n",
    "        model_id = result['dataset_id']\n",
    "        model_info = {\n",
    "            'model': result['model'],\n",
    "            'scaler': result['scaler'],\n",
    "            'cv_scores': result['cv_scores'],\n",
    "            'mean_scores': result['mean_scores'],\n",
    "            'std_scores': result['std_scores']\n",
    "        }\n",
    "        \n",
    "        # 保存单个模型\n",
    "        model_filename = f\"{model_dir}/model_{model_id}.pkl\"\n",
    "        joblib.dump(model_info, model_filename)\n",
    "        individual_models[f'model_{model_id}'] = model_filename\n",
    "        print(f\"  ✅ 模型 {model_id} 已保存: {model_filename}\")\n",
    "    \n",
    "    # 2. 保存集成模型预测器（使用全局定义的类）\n",
    "    print(\"\\\\n保存集成模型预测器...\")\n",
    "    \n",
    "    # 创建集成预测器（使用已定义的全局EnsemblePredictor类）\n",
    "    ensemble_predictor = EnsemblePredictor(\n",
    "        models=[{'model': r['model'], 'scaler': r['scaler']} for r in training_results],\n",
    "        feature_names=feature_names,\n",
    "        threshold_strategies=strategies\n",
    "    )\n",
    "    \n",
    "    # 保存集成预测器\n",
    "    ensemble_filename = f\"{model_dir}/ensemble_predictor.pkl\"\n",
    "    joblib.dump(ensemble_predictor, ensemble_filename)\n",
    "    print(f\"  ✅ 集成预测器已保存: {ensemble_filename}\")\n",
    "    \n",
    "    # 3. 保存模型元信息\n",
    "    print(\"\\\\n保存模型元信息...\")\n",
    "    model_metadata = {\n",
    "        'timestamp': timestamp,\n",
    "        'model_type': 'Multiple Undersampling Logistic Regression Ensemble',\n",
    "        'n_models': len(training_results),\n",
    "        'feature_names': feature_names,\n",
    "        'n_features': len(feature_names),\n",
    "        'training_strategy': '149正样本 + 10次随机149负样本',\n",
    "        'ensemble_results': ensemble_results,\n",
    "        'threshold_strategies': {name: result for name, result in strategies},\n",
    "        'individual_model_files': individual_models,\n",
    "        'ensemble_file': ensemble_filename,\n",
    "        'model_directory': model_dir\n",
    "    }\n",
    "    \n",
    "    metadata_filename = f\"{model_dir}/model_metadata.pkl\"\n",
    "    with open(metadata_filename, 'wb') as f:\n",
    "        pickle.dump(model_metadata, f)\n",
    "    print(f\"  ✅ 模型元信息已保存: {metadata_filename}\")\n",
    "    \n",
    "    # 4. 保存使用说明\n",
    "    print(\"\\\\n生成使用说明...\")\n",
    "    usage_guide = f'''# 模型使用指南\n",
    "保存时间: {timestamp}\n",
    "\n",
    "## 文件说明:\n",
    "- ensemble_predictor.pkl: 集成预测器（推荐使用）\n",
    "- model_1.pkl ~ model_10.pkl: 10个单独的逻辑回归模型\n",
    "- model_metadata.pkl: 模型元信息和性能数据\n",
    "\n",
    "## 使用示例:\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 加载集成预测器\n",
    "ensemble_model = joblib.load('{ensemble_filename}')\n",
    "\n",
    "# 准备新数据（确保特征顺序与训练时一致）\n",
    "# X_new = your_new_data[feature_names]\n",
    "\n",
    "# 方法1: 使用默认阈值预测\n",
    "predictions = ensemble_model.predict(X_new)\n",
    "probabilities = ensemble_model.predict_proba(X_new)\n",
    "\n",
    "# 方法2: 使用特定策略预测\n",
    "predictions, probabilities = ensemble_model.predict_with_strategy(X_new, '最高召回率策略')\n",
    "\n",
    "# 可用策略:\n",
    "# - '最高召回率策略': 最大化转化用户识别\n",
    "# - '最高F1策略': 平衡精确率和召回率  \n",
    "# - '最高精确率策略': 最小化误判\n",
    "# - '业务平衡策略': 适中的业务平衡点\n",
    "```\n",
    "\n",
    "## 性能总结:\n",
    "'''\n",
    "    \n",
    "    for name, result in strategies:\n",
    "        usage_guide += f\"- {name}: 阈值{result['threshold']:.1f}, 召回率{result['recall']:.3f}, 精确率{result['precision']:.3f}\\\\n\"\n",
    "    \n",
    "    with open(f\"{model_dir}/使用指南.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(usage_guide)\n",
    "    print(f\"  ✅ 使用指南已保存: {model_dir}/使用指南.txt\")\n",
    "    \n",
    "    print(f\"\\\\n🎉 所有模型文件已保存到目录: {model_dir}\")\n",
    "    print(f\"📁 目录包含:\")\n",
    "    for file in os.listdir(model_dir):\n",
    "        file_path = os.path.join(model_dir, file)\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"   📄 {file} ({file_size:.1f} KB)\")\n",
    "    \n",
    "    return model_dir, ensemble_predictor\n",
    "\n",
    "# 重新执行模型保存（使用修复版）\n",
    "print(\"🔧 使用修复版函数重新保存模型...\")\n",
    "\n",
    "# 显示混淆矩阵（如果还没有显示）\n",
    "try:\n",
    "    final_strategies\n",
    "    print(\"✅ 混淆矩阵已显示\")\n",
    "except NameError:\n",
    "    print(\"⚠️  重新生成混淆矩阵...\")\n",
    "    final_strategies = display_final_confusion_matrices(ensemble_results, y_full_test)\n",
    "\n",
    "# 保存模型（使用修复版函数）\n",
    "model_directory_fixed, saved_ensemble_model_fixed = save_ensemble_model_fixed(\n",
    "    training_results, ensemble_results, feature_names, final_strategies\n",
    ")\n",
    "\n",
    "print(f\"\\\\n✅ 模型保存完成！目录: {model_directory_fixed}\")\n",
    "print(f\"🎉 现在可以安全地加载和使用模型了！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70879b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化分析和结果汇总\n",
    "# =======================\n",
    "\n",
    "def plot_comprehensive_analysis(training_results, ensemble_results, y_full_test):\n",
    "    \"\"\"\n",
    "    绘制综合分析图表\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. 交叉验证性能对比\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    dataset_ids = [result['dataset_id'] for result in training_results]\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if i < 5:  # 只绘制前5个指标\n",
    "            mean_values = [result['mean_scores'][metric] for result in training_results]\n",
    "            std_values = [result['std_scores'][metric] for result in training_results]\n",
    "            \n",
    "            if metric == 'recall':\n",
    "                axes[0, 0].errorbar(dataset_ids, mean_values, yerr=std_values, \n",
    "                                  marker='o', label=metric, linewidth=2, markersize=6)\n",
    "            elif metric == 'precision':\n",
    "                axes[0, 1].errorbar(dataset_ids, mean_values, yerr=std_values, \n",
    "                                  marker='s', label=metric, linewidth=2, markersize=6)\n",
    "            elif metric == 'f1':\n",
    "                axes[0, 2].errorbar(dataset_ids, mean_values, yerr=std_values, \n",
    "                                  marker='^', label=metric, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[0, 0].set_title('各数据集召回率对比', fontsize=14)\n",
    "    axes[0, 0].set_xlabel('数据集ID')\n",
    "    axes[0, 0].set_ylabel('召回率')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    axes[0, 1].set_title('各数据集精确率对比', fontsize=14)\n",
    "    axes[0, 1].set_xlabel('数据集ID')\n",
    "    axes[0, 1].set_ylabel('精确率')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    axes[0, 2].set_title('各数据集F1分数对比', fontsize=14)\n",
    "    axes[0, 2].set_xlabel('数据集ID')\n",
    "    axes[0, 2].set_ylabel('F1分数')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # 2. 阈值性能曲线\n",
    "    thresholds = [result['threshold'] for result in ensemble_results['threshold_results']]\n",
    "    recalls = [result['recall'] for result in ensemble_results['threshold_results']]\n",
    "    precisions = [result['precision'] for result in ensemble_results['threshold_results']]\n",
    "    f1s = [result['f1'] for result in ensemble_results['threshold_results']]\n",
    "    \n",
    "    axes[1, 0].plot(thresholds, recalls, 'o-', label='召回率', linewidth=2, markersize=6)\n",
    "    axes[1, 0].plot(thresholds, precisions, 's-', label='精确率', linewidth=2, markersize=6)\n",
    "    axes[1, 0].plot(thresholds, f1s, '^-', label='F1分数', linewidth=2, markersize=6)\n",
    "    axes[1, 0].set_xlabel('预测阈值')\n",
    "    axes[1, 0].set_ylabel('性能指标')\n",
    "    axes[1, 0].set_title('集成模型阈值性能曲线', fontsize=14)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 转化用户识别数量\n",
    "    identified_counts = [result['identified_converted'] for result in ensemble_results['threshold_results']]\n",
    "    total_converted = y_full_test.sum()\n",
    "    \n",
    "    axes[1, 1].bar(range(len(thresholds)), identified_counts, alpha=0.7, color='skyblue')\n",
    "    axes[1, 1].axhline(y=total_converted, color='red', linestyle='--', \n",
    "                      label=f'总转化用户数: {total_converted}')\n",
    "    axes[1, 1].set_xlabel('阈值索引')\n",
    "    axes[1, 1].set_ylabel('识别的转化用户数')\n",
    "    axes[1, 1].set_title('不同阈值识别的转化用户数', fontsize=14)\n",
    "    axes[1, 1].set_xticks(range(len(thresholds)))\n",
    "    axes[1, 1].set_xticklabels([f'{t:.1f}' for t in thresholds], rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 混淆矩阵 (最佳F1阈值)\n",
    "    best_f1_idx = np.argmax(f1s)\n",
    "    best_threshold = thresholds[best_f1_idx]\n",
    "    best_predictions = (ensemble_results['ensemble_proba'] >= best_threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_full_test, best_predictions)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 2])\n",
    "    axes[1, 2].set_title(f'混淆矩阵 (阈值={best_threshold:.1f})', fontsize=14)\n",
    "    axes[1, 2].set_xlabel('预测值')\n",
    "    axes[1, 2].set_ylabel('实际值')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 执行可视化和报告生成\n",
    "plot_comprehensive_analysis(training_results, ensemble_results, y_full_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37c3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ff245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征重要性分析（包含置信度计算）\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def analyze_feature_importance_with_confidence(training_results, X_test, y_test, feature_names):\n",
    "    \"\"\"\n",
    "    分析特征重要性并计算置信度\n",
    "    \n",
    "    参数:\n",
    "    - training_results: 训练结果列表\n",
    "    - X_test: 测试特征\n",
    "    - y_test: 测试标签  \n",
    "    - feature_names: 特征名称列表\n",
    "    \n",
    "    返回:\n",
    "    - 特征重要性分析结果\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 开始特征重要性分析...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 收集所有模型的系数重要性\n",
    "    coefficients_matrix = []\n",
    "    \n",
    "    for result in training_results:\n",
    "        model = result['model']\n",
    "        # 获取逻辑回归系数（绝对值表示重要性）\n",
    "        coeffs = np.abs(model.coef_[0])\n",
    "        coefficients_matrix.append(coeffs)\n",
    "    \n",
    "    coefficients_matrix = np.array(coefficients_matrix)\n",
    "    \n",
    "    # 2. 计算系数重要性统计\n",
    "    coef_importance = {\n",
    "        'feature': feature_names,\n",
    "        'mean_importance': np.mean(coefficients_matrix, axis=0),\n",
    "        'std_importance': np.std(coefficients_matrix, axis=0),\n",
    "        'min_importance': np.min(coefficients_matrix, axis=0),\n",
    "        'max_importance': np.max(coefficients_matrix, axis=0)\n",
    "    }\n",
    "    \n",
    "    # 计算置信区间 (95%)\n",
    "    confidence_intervals = []\n",
    "    for i in range(len(feature_names)):\n",
    "        feature_coeffs = coefficients_matrix[:, i]\n",
    "        # 计算95%置信区间\n",
    "        ci_lower, ci_upper = stats.t.interval(\n",
    "            confidence=0.95,\n",
    "            df=len(feature_coeffs)-1,\n",
    "            loc=np.mean(feature_coeffs),\n",
    "            scale=stats.sem(feature_coeffs)\n",
    "        )\n",
    "        confidence_intervals.append((ci_lower, ci_upper))\n",
    "    \n",
    "    coef_importance['confidence_interval'] = confidence_intervals\n",
    "    \n",
    "    # 3. 计算变异系数（稳定性指标）\n",
    "    cv_scores = coef_importance['std_importance'] / (coef_importance['mean_importance'] + 1e-10)\n",
    "    coef_importance['coefficient_variation'] = cv_scores\n",
    "    \n",
    "    # 4. 使用置换重要性验证（取前3个模型进行计算，节省时间）\n",
    "    print(\"🔄 计算置换重要性（使用10个模型验证）...\")\n",
    "    \n",
    "    permutation_scores = []\n",
    "    for i, result in enumerate(training_results[:10]):  # 只用10个模型\n",
    "        model = result['model']\n",
    "        scaler = result['scaler']\n",
    "        \n",
    "        # 标准化测试数据\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # 计算置换重要性\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test_scaled, y_test,\n",
    "            scoring='roc_auc',\n",
    "            n_repeats=5,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        permutation_scores.append(perm_importance.importances_mean)\n",
    "        print(f\"   模型 {i+1}/10 完成\")\n",
    "    \n",
    "    # 计算置换重要性统计\n",
    "    permutation_matrix = np.array(permutation_scores)\n",
    "    perm_importance = {\n",
    "        'mean_importance': np.mean(permutation_matrix, axis=0),\n",
    "        'std_importance': np.std(permutation_matrix, axis=0)\n",
    "    }\n",
    "    \n",
    "    # 5. 创建综合重要性数据框\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coef_importance': coef_importance['mean_importance'],\n",
    "        'coef_std': coef_importance['std_importance'],\n",
    "        'coef_cv': coef_importance['coefficient_variation'],\n",
    "        'confidence_lower': [ci[0] for ci in coef_importance['confidence_interval']],\n",
    "        'confidence_upper': [ci[1] for ci in coef_importance['confidence_interval']],\n",
    "        'perm_importance': perm_importance['mean_importance'],\n",
    "        'perm_std': perm_importance['std_importance']\n",
    "    })\n",
    "    \n",
    "    # 计算综合重要性得分（系数重要性 + 置换重要性的标准化组合）\n",
    "    coef_norm = (importance_df['coef_importance'] - importance_df['coef_importance'].min()) / \\\n",
    "                (importance_df['coef_importance'].max() - importance_df['coef_importance'].min() + 1e-10)\n",
    "    perm_norm = (importance_df['perm_importance'] - importance_df['perm_importance'].min()) / \\\n",
    "                (importance_df['perm_importance'].max() - importance_df['perm_importance'].min() + 1e-10)\n",
    "    \n",
    "    importance_df['combined_importance'] = (coef_norm + perm_norm) / 2\n",
    "    \n",
    "    # 计算稳定性得分（变异系数越小，稳定性越高）\n",
    "    importance_df['stability_score'] = 1 / (1 + importance_df['coef_cv'])\n",
    "    \n",
    "    # 按综合重要性排序\n",
    "    importance_df = importance_df.sort_values('combined_importance', ascending=False)\n",
    "    \n",
    "    print(\"✅ 特征重要性分析完成！\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def plot_feature_importance_analysis(importance_df, top_n=20):\n",
    "    \"\"\"\n",
    "    可视化特征重要性分析结果\n",
    "    \"\"\"\n",
    "    \n",
    "    # 取前N个最重要的特征\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    # 创建图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # 1. 系数重要性排序（带置信区间）\n",
    "    ax1 = axes[0, 0]\n",
    "    y_pos = np.arange(len(top_features))\n",
    "    \n",
    "    # 绘制误差条\n",
    "    ax1.barh(y_pos, top_features['coef_importance'], \n",
    "             xerr=[top_features['coef_importance'] - top_features['confidence_lower'],\n",
    "                   top_features['confidence_upper'] - top_features['coef_importance']],\n",
    "             alpha=0.7, capsize=3)\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "    ax1.set_xlabel('系数重要性（绝对值）')\n",
    "    ax1.set_title(f'前{top_n}个特征的系数重要性（带95%置信区间）', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 置换重要性\n",
    "    ax2 = axes[0, 1]\n",
    "    y_pos = np.arange(len(top_features))\n",
    "    \n",
    "    bars = ax2.barh(y_pos, top_features['perm_importance'], \n",
    "                    xerr=top_features['perm_std'],\n",
    "                    alpha=0.7, capsize=3, color='orange')\n",
    "    \n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "    ax2.set_xlabel('置换重要性（ROC AUC损失）')\n",
    "    ax2.set_title(f'前{top_n}个特征的置换重要性', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 综合重要性得分\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    # 使用颜色表示稳定性\n",
    "    colors = plt.cm.RdYlGn(top_features['stability_score'])\n",
    "    bars = ax3.barh(y_pos, top_features['combined_importance'], color=colors, alpha=0.8)\n",
    "    \n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels(top_features['feature'], fontsize=10)\n",
    "    ax3.set_xlabel('综合重要性得分')\n",
    "    ax3.set_title(f'前{top_n}个特征的综合重要性得分\\\\n（颜色深浅表示稳定性）', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加颜色条\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, \n",
    "                              norm=plt.Normalize(vmin=top_features['stability_score'].min(), \n",
    "                                               vmax=top_features['stability_score'].max()))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax3)\n",
    "    cbar.set_label('稳定性得分')\n",
    "    \n",
    "    # 4. 重要性相关性分析\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # 散点图：系数重要性 vs 置换重要性\n",
    "    scatter = ax4.scatter(top_features['coef_importance'], \n",
    "                         top_features['perm_importance'],\n",
    "                         c=top_features['stability_score'],\n",
    "                         s=100, alpha=0.7, cmap='RdYlGn')\n",
    "    \n",
    "    # 添加特征名称标注（仅显示前10个）\n",
    "    for i, (idx, row) in enumerate(top_features.head(10).iterrows()):\n",
    "        ax4.annotate(row['feature'], \n",
    "                    (row['coef_importance'], row['perm_importance']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    ax4.set_xlabel('系数重要性')\n",
    "    ax4.set_ylabel('置换重要性')\n",
    "    ax4.set_title('重要性方法一致性分析', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 计算相关系数\n",
    "    correlation = np.corrcoef(top_features['coef_importance'], \n",
    "                             top_features['perm_importance'])[0, 1]\n",
    "    ax4.text(0.05, 0.95, f'相关系数: {correlation:.3f}', \n",
    "             transform=ax4.transAxes, fontsize=12,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax4, label='稳定性得分')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def generate_feature_importance_report(importance_df):\n",
    "    \"\"\"\n",
    "    生成特征重要性分析报告\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"🎯 特征重要性分析报告\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Top 10 最重要特征\n",
    "    print(\"\\\\n🏆 Top 10 最重要特征:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    top_10 = importance_df.head(10)\n",
    "    for i, (idx, row) in enumerate(top_10.iterrows(), 1):\n",
    "        print(f\"{i:2d}. {row['feature']}\")\n",
    "        print(f\"     综合重要性: {row['combined_importance']:.4f}\")\n",
    "        print(f\"     系数重要性: {row['coef_importance']:.4f} (±{row['coef_std']:.4f})\")\n",
    "        print(f\"     置换重要性: {row['perm_importance']:.4f} (±{row['perm_std']:.4f})\")\n",
    "        print(f\"     稳定性得分: {row['stability_score']:.4f}\")\n",
    "        print(f\"     95%置信区间: [{row['confidence_lower']:.4f}, {row['confidence_upper']:.4f}]\")\n",
    "        print()\n",
    "    \n",
    "    # 2. 特征分类分析\n",
    "    print(\"\\\\n📊 特征分类分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 根据特征名称分类\n",
    "    feature_categories = {\n",
    "        '通话相关': [],\n",
    "        '转化模式': [],\n",
    "        '渠道相关': [],\n",
    "        '时间相关': [],\n",
    "        '其他': []\n",
    "    }\n",
    "    \n",
    "    for _, row in importance_df.iterrows():\n",
    "        feature = row['feature']\n",
    "        if any(keyword in feature for keyword in ['通话', '打通', '完播', '拨打']):\n",
    "            feature_categories['通话相关'].append((feature, row['combined_importance']))\n",
    "        elif any(keyword in feature for keyword in ['转化', '模式', '高']):\n",
    "            feature_categories['转化模式'].append((feature, row['combined_importance']))\n",
    "        elif any(keyword in feature for keyword in ['渠道', '白名单']):\n",
    "            feature_categories['渠道相关'].append((feature, row['combined_importance']))\n",
    "        elif any(keyword in feature for keyword in ['时段', '工作日', '周']):\n",
    "            feature_categories['时间相关'].append((feature, row['combined_importance']))\n",
    "        else:\n",
    "            feature_categories['其他'].append((feature, row['combined_importance']))\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            avg_importance = np.mean([imp for _, imp in features])\n",
    "            print(f\"{category}: {len(features)}个特征, 平均重要性: {avg_importance:.4f}\")\n",
    "            # 显示该类别最重要的特征\n",
    "            top_feature = max(features, key=lambda x: x[1])\n",
    "            print(f\"   最重要: {top_feature[0]} ({top_feature[1]:.4f})\")\n",
    "    \n",
    "    # 3. 稳定性分析\n",
    "    print(\"\\\\n📈 模型稳定性分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    high_stability = importance_df[importance_df['stability_score'] > 0.8]\n",
    "    medium_stability = importance_df[(importance_df['stability_score'] > 0.6) & \n",
    "                                   (importance_df['stability_score'] <= 0.8)]\n",
    "    low_stability = importance_df[importance_df['stability_score'] <= 0.6]\n",
    "    \n",
    "    print(f\"高稳定性特征 (>0.8): {len(high_stability)}个\")\n",
    "    print(f\"中等稳定性特征 (0.6-0.8): {len(medium_stability)}个\")\n",
    "    print(f\"低稳定性特征 (≤0.6): {len(low_stability)}个\")\n",
    "    \n",
    "    if len(low_stability) > 0:\n",
    "        print(f\"\\\\n⚠️  低稳定性特征:\")\n",
    "        for _, row in low_stability.head(5).iterrows():\n",
    "            print(f\"   {row['feature']}: 稳定性={row['stability_score']:.3f}\")\n",
    "    \n",
    "    # 4. 置信度分析\n",
    "    print(\"\\\\n🎯 置信度分析:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 计算置信区间宽度\n",
    "    confidence_width = importance_df['confidence_upper'] - importance_df['confidence_lower']\n",
    "    \n",
    "    narrow_ci = importance_df[confidence_width < confidence_width.quantile(0.25)]\n",
    "    wide_ci = importance_df[confidence_width > confidence_width.quantile(0.75)]\n",
    "    \n",
    "    print(f\"高置信度特征 (窄置信区间): {len(narrow_ci)}个\")\n",
    "    print(f\"低置信度特征 (宽置信区间): {len(wide_ci)}个\")\n",
    "    print(f\"平均置信区间宽度: {confidence_width.mean():.4f}\")\n",
    "    \n",
    "    # 5. 业务解释\n",
    "    print(\"\\\\n💼 业务解释:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    top_3 = importance_df.head(3)\n",
    "    print(\"最重要的3个特征业务含义:\")\n",
    "    for i, (_, row) in enumerate(top_3.iterrows(), 1):\n",
    "        feature = row['feature']\n",
    "        print(f\"{i}. {feature}\")\n",
    "        \n",
    "        # 简单的业务解释\n",
    "        if '转化模式得分' in feature:\n",
    "            print(\"   → 综合转化行为模式评分，是最强的转化预测指标\")\n",
    "        elif '通话' in feature:\n",
    "            print(\"   → 通话相关行为，反映客户参与度和兴趣程度\")\n",
    "        elif '渠道' in feature or '白名单' in feature:\n",
    "            print(\"   → 客户来源渠道，不同渠道的客户转化倾向不同\")\n",
    "        elif 'AI' in feature or '人工' in feature:\n",
    "            print(\"   → 客户意向评估，直接反映转化可能性\")\n",
    "        else:\n",
    "            print(\"   → 其他重要的客户行为或属性特征\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"✅ 特征重要性分析报告生成完成！\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# 执行特征重要性分析\n",
    "print(\"🚀 开始执行特征重要性分析...\")\n",
    "importance_results = analyze_feature_importance_with_confidence(\n",
    "    training_results, \n",
    "    X_full_test, \n",
    "    y_full_test, \n",
    "    feature_names\n",
    ")\n",
    "\n",
    "# 可视化分析结果\n",
    "plot_feature_importance_analysis(importance_results, top_n=20)\n",
    "\n",
    "# 生成详细报告\n",
    "final_importance_df = generate_feature_importance_report(importance_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存特征重要性结果\n",
    "# =======================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# 创建保存目录\n",
    "save_dir = \"特征重要性分析结果\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 生成时间戳\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 保存详细的重要性数据\n",
    "importance_file = f\"{save_dir}/特征重要性分析_{timestamp}.xlsx\"\n",
    "with pd.ExcelWriter(importance_file, engine='openpyxl') as writer:\n",
    "    # 保存完整的重要性分析结果\n",
    "    final_importance_df.to_excel(writer, sheet_name='特征重要性排序', index=False)\n",
    "    \n",
    "    # 创建Top 10汇总\n",
    "    top_10_summary = final_importance_df.head(10)[['feature', 'combined_importance', \n",
    "                                                    'coef_importance', 'perm_importance', \n",
    "                                                    'stability_score']].copy()\n",
    "    top_10_summary.columns = ['特征名称', '综合重要性', '系数重要性', '置换重要性', '稳定性得分']\n",
    "    top_10_summary.to_excel(writer, sheet_name='Top10特征汇总', index=False)\n",
    "    \n",
    "    # 创建分类汇总\n",
    "    feature_categories_summary = []\n",
    "    feature_categories = {\n",
    "        '通话相关': [],\n",
    "        '转化模式': [],\n",
    "        '渠道相关': [],\n",
    "        '时间相关': [],\n",
    "        '其他': []\n",
    "    }\n",
    "    \n",
    "    for _, row in final_importance_df.iterrows():\n",
    "        feature = row['feature']\n",
    "        if any(keyword in feature for keyword in ['通话', '打通', '完播', '拨打']):\n",
    "            feature_categories['通话相关'].append(row['combined_importance'])\n",
    "        elif any(keyword in feature for keyword in ['转化', '模式', '高']):\n",
    "            feature_categories['转化模式'].append(row['combined_importance'])\n",
    "        elif any(keyword in feature for keyword in ['渠道', '白名单']):\n",
    "            feature_categories['渠道相关'].append(row['combined_importance'])\n",
    "        elif any(keyword in feature for keyword in ['时段', '工作日', '周']):\n",
    "            feature_categories['时间相关'].append(row['combined_importance'])\n",
    "        else:\n",
    "            feature_categories['其他'].append(row['combined_importance'])\n",
    "    \n",
    "    for category, importances in feature_categories.items():\n",
    "        if importances:\n",
    "            feature_categories_summary.append({\n",
    "                '特征类别': category,\n",
    "                '特征数量': len(importances),\n",
    "                '平均重要性': np.mean(importances),\n",
    "                '最高重要性': np.max(importances),\n",
    "                '最低重要性': np.min(importances)\n",
    "            })\n",
    "    \n",
    "    category_df = pd.DataFrame(feature_categories_summary)\n",
    "    category_df.to_excel(writer, sheet_name='特征分类汇总', index=False)\n",
    "\n",
    "print(f\"\\\\n💾 特征重要性分析结果已保存到: {importance_file}\")\n",
    "\n",
    "# 创建简化的重要性字典（供其他模块使用）\n",
    "importance_dict = dict(zip(final_importance_df['feature'], \n",
    "                          final_importance_df['combined_importance']))\n",
    "\n",
    "# 显示保存的文件信息\n",
    "print(f\"\\\\n📂 保存的文件包含以下内容:\")\n",
    "print(f\"   📄 特征重要性排序: 包含{len(final_importance_df)}个特征的完整分析\")\n",
    "print(f\"   📄 Top10特征汇总: 最重要的10个特征详情\")  \n",
    "print(f\"   📄 特征分类汇总: 按业务类别归类的重要性统计\")\n",
    "\n",
    "# 快速查看Top 5特征及其置信度\n",
    "print(f\"\\\\n🏆 Top 5 特征重要性快速预览:\")\n",
    "print(\"=\" * 70)\n",
    "for i, (_, row) in enumerate(final_importance_df.head(5).iterrows(), 1):\n",
    "    ci_width = row['confidence_upper'] - row['confidence_lower']\n",
    "    print(f\"{i}. {row['feature']}\")\n",
    "    print(f\"   重要性: {row['combined_importance']:.4f}\")\n",
    "    print(f\"   置信区间: [{row['confidence_lower']:.4f}, {row['confidence_upper']:.4f}] (宽度: {ci_width:.4f})\")\n",
    "    print(f\"   稳定性: {row['stability_score']:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"✅ 特征重要性分析完成！可以在其他分析中使用 `importance_dict` 变量\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_feature_importanc = final_validation_results['shap_result']['feature_importance'] \n",
    "shap_feature_importanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_validation_results['shap_result']['feature_names']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb33339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 提取 shap_values 数组，计算绝对值并按特征求均值\n",
    "shap_importance = final_validation_results['shap_result']['feature_importance']   # 长度为 n_features\n",
    "shap_features = final_validation_results['shap_result']['feature_names']  \n",
    "# 2. 构建新的 DataFrame 或合并到已有 DataFrame\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_importance': shap_feature_importanc\n",
    "})\n",
    "shap_df\n",
    "# 如果你已有 importance_df，做 merge：\n",
    "final_importance_df = final_importance_df.merge(shap_df, on='feature', how='left')\n",
    "final_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53967fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际置信区间分析工具\n",
    "# =====================\n",
    "\n",
    "def analyze_confidence_intervals(importance_df):\n",
    "    \"\"\"\n",
    "    分析实际数据的置信区间并提供建议\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 您的数据置信区间分析\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 计算置信区间宽度\n",
    "    importance_df['ci_width'] = importance_df['confidence_upper'] - importance_df['confidence_lower']\n",
    "    #相对区间宽度\n",
    "    importance_df['ci_width']= importance_df['ci_width'] / (importance_df['combined_importance'].abs() + 1e-8)\n",
    "    # 分类特征\n",
    "    high_confidence = importance_df[importance_df['ci_width'] <= 0.02]\n",
    "    medium_confidence = importance_df[(importance_df['ci_width'] > 0.02) & \n",
    "                                    (importance_df['ci_width'] <= 0.15)]\n",
    "    low_confidence = importance_df[importance_df['ci_width'] > 0.15]\n",
    "    \n",
    "    print(f\"信度分类结果:\")\n",
    "    print(f\" 高置信度特征: {len(high_confidence)}个 (宽度 ≤ 0.02)\")\n",
    "    print(f\" 中等置信度特征: {len(medium_confidence)}个 (宽度 0.02-0.15)\")\n",
    "    print(f\" 低置信度特征: {len(low_confidence)}个 (宽度 > 0.15)\")\n",
    "    \n",
    "    print(f\"\\n🏆 最可信的Top 10特征 (置信区间最窄):\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # 按置信区间宽度排序，找出最可信的特征\n",
    "    most_reliable = importance_df.nsmallest(10, 'ci_width')\n",
    "    \n",
    "    for i, (_, row) in enumerate(most_reliable.iterrows(), 1):\n",
    "        print(f\"{i}. {row['feature']}\")\n",
    "        print(f\"   重要性: {row['combined_importance']:.4f}\")\n",
    "        print(f\"   置信区间: [{row['confidence_lower']:.4f}, {row['confidence_upper']:.4f}]\")\n",
    "        print(f\"   区间宽度: {row['ci_width']:.4f} ✅\")\n",
    "        print()\n",
    "    \n",
    "    if len(low_confidence) > 0:\n",
    "        print(f\"需要注意的低置信度特征:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        for i, (_, row) in enumerate(low_confidence.head(3).iterrows(), 1):\n",
    "            print(f\"{i}. {row['feature']}\")\n",
    "            print(f\"   重要性: {row['combined_importance']:.4f}\")\n",
    "            print(f\"   置信区间: [{row['confidence_lower']:.4f}, {row['confidence_upper']:.4f}]\")\n",
    "            print(f\"   区间宽度: {row['ci_width']:.4f}\")\n",
    "            print(f\"   → 建议: 这个特征的重要性不稳定，使用时需谨慎\")\n",
    "            print()\n",
    "    \n",
    "    # 综合建议\n",
    "    print(f\"综合建议:\")\n",
    "    print(\"-\" * 15)\n",
    "    \n",
    "    reliable_important = importance_df[\n",
    "        (importance_df['combined_importance'] > 0.5) & \n",
    "        (importance_df['ci_width'] <= 0.05)\n",
    "    ]\n",
    "    \n",
    "    if len(reliable_important) > 0:\n",
    "        print(f\"推荐核心特征 ({len(reliable_important)}个): 重要性高且可信度高\")\n",
    "        for _, row in reliable_important.head(3).iterrows():\n",
    "            print(f\"   • {row['feature']}\")\n",
    "    \n",
    "    unreliable_features = importance_df[importance_df['ci_width'] > 0.1]\n",
    "    if len(unreliable_features) > 0:\n",
    "        print(f\"建议重新评估 ({len(unreliable_features)}个): 置信区间过宽\")\n",
    "        print(f\"   → 可能需要更多数据或特征工程优化\")\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def create_confidence_summary_table(importance_df):\n",
    "    \"\"\"\n",
    "    创建置信度汇总表\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"置信度汇总表\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 选择关键列并重命名\n",
    "    summary_cols = ['feature', 'combined_importance', 'confidence_lower', \n",
    "                   'confidence_upper', 'ci_width', 'stability_score']\n",
    "    \n",
    "    summary_df = importance_df[summary_cols].head(10).copy()\n",
    "    \n",
    "    # 重命名列\n",
    "    summary_df.columns = ['特征名称', '综合重要性', '置信下限', '置信上限', \n",
    "                         '区间宽度', '稳定性得分']\n",
    "    \n",
    "    # 添加可信度评级\n",
    "    def get_confidence_rating(width):\n",
    "        if width <= 0.02:\n",
    "            return \" 高\"\n",
    "        elif width <= 0.15:\n",
    "            return \" 中\"\n",
    "        else:\n",
    "            return \" 低\"\n",
    "    \n",
    "    summary_df['可信度'] = summary_df['区间宽度'].apply(get_confidence_rating)\n",
    "    \n",
    "    # 格式化数值\n",
    "    for col in ['综合重要性', '置信下限', '置信上限', '区间宽度', '稳定性得分']:\n",
    "        summary_df[col] = summary_df[col].round(4)\n",
    "    \n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# 检查是否有importance结果可以分析\n",
    "try:\n",
    "    if 'final_importance_df' in locals():\n",
    "        print(\"🚀 分析您的实际特征重要性置信区间...\")\n",
    "        \n",
    "        # 分析置信区间\n",
    "        analyzed_df = analyze_confidence_intervals(final_importance_df)\n",
    "        \n",
    "        # 创建汇总表\n",
    "        summary_table = create_confidence_summary_table(analyzed_df)\n",
    "        \n",
    "    else:\n",
    "        print(\" 请先运行特征重要性分析代码以获得 final_importance_df\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 分析过程中出错: {e}\")\n",
    "    print(\" 请确保已经运行了特征重要性分析的代码\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制最可信的Top 10特征图表\n",
    "# ============================\n",
    "\n",
    "def plot_most_reliable_top10_features(importance_df):\n",
    "    \"\"\"\n",
    "    绘制最可信的Top 10特征的详细图表\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"绘制最可信的Top 10特征...\")\n",
    "    \n",
    "    # 计算置信区间宽度\n",
    "    if 'ci_width' not in importance_df.columns:\n",
    "        importance_df['ci_width'] = importance_df['confidence_upper'] - importance_df['confidence_lower']\n",
    "        importance_df['ci_width']=  (importance_df['confidence_upper'] - importance_df['confidence_lower'])/ (importance_df['combined_importance'].abs() + 1e-8)\n",
    "    # 按shap重要性排序，选择最可信的Top 10\n",
    "    importance_df['shap_importance']\n",
    "    most_reliable_top10 = importance_df.nsmallest(10, 'shap_importance').copy()\n",
    "    \n",
    "    # 创建大图表\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # 设置颜色方案\n",
    "    colors = plt.cm.RdYlGn(np.linspace(0.3, 1.0, 10))  # 绿色系表示可信度高\n",
    "    \n",
    "    # 1. 特征重要性排序（带置信区间）\n",
    "    ax1 = axes[0, 0]\n",
    "    y_pos = np.arange(len(most_reliable_top10))\n",
    "    \n",
    "    # 绘制条形图\n",
    "    bars = ax1.barh(y_pos, most_reliable_top10['combined_importance'], \n",
    "                   color=colors, alpha=0.8, height=0.6)\n",
    "    \n",
    "    # 绘制置信区间\n",
    "    for i, (_, row) in enumerate(most_reliable_top10.iterrows()):\n",
    "        # 标注置信区间宽度\n",
    "        ax1.text(row['combined_importance'] + 0.002, i, \n",
    "                f'{row[\"ci_width\"]:.4f}', \n",
    "                verticalalignment='center', fontsize=9, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.7))\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(most_reliable_top10['feature'], fontsize=11)\n",
    "    ax1.set_xlabel('综合重要性得分', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('SHAPTop 10特征重要性排序(表示95%置信区间宽度,越小越可信)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, max(most_reliable_top10['combined_importance']) * 1.2)\n",
    "    \n",
    "    # 2. 置信区间宽度比较\n",
    "    ax2 = axes[0, 1]\n",
    "    \n",
    "    bars2 = ax2.bar(range(len(most_reliable_top10)), most_reliable_top10['ci_width'], \n",
    "                   color=colors, alpha=0.8)\n",
    "    \n",
    "    ax2.set_xticks(range(len(most_reliable_top10)))\n",
    "    ax2.set_xticklabels([f'特征{i+1}' for i in range(len(most_reliable_top10))], \n",
    "                       rotation=45, ha='right')\n",
    "    ax2.set_ylabel('置信区间宽度', fontsize=12, fontweight='bold')\n",
    "    ax2.set_title('Top 10特征的置信区间宽度(越小表示越可信)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 在每个柱子上标注数值\n",
    "    for i, (bar, width) in enumerate(zip(bars2, most_reliable_top10['ci_width'])):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.0005, \n",
    "                f'{width:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 添加分界线（可信度标准）\n",
    "    ax2.axhline(y=0.02, color='green', linestyle='--', alpha=0.7, linewidth=2,\n",
    "               label='高可信度线 (< 0.02)')\n",
    "    ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, linewidth=2,\n",
    "               label='中等可信度线 (< 0.05)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. 稳定性得分对比\n",
    "    ax3 = axes[1, 0]\n",
    "    \n",
    "    bars3 = ax3.bar(range(len(most_reliable_top10)), most_reliable_top10['stability_score'], \n",
    "                   color=colors, alpha=0.8)\n",
    "    \n",
    "    ax3.set_xticks(range(len(most_reliable_top10)))\n",
    "    ax3.set_xticklabels([f'特征{i+1}' for i in range(len(most_reliable_top10))], \n",
    "                       rotation=45, ha='right')\n",
    "    ax3.set_ylabel('稳定性得分', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title('Top 10特征的稳定性得分(越高表示越稳定)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1.1)\n",
    "    \n",
    "    # 在每个柱子上标注数值\n",
    "    for i, (bar, score) in enumerate(zip(bars3, most_reliable_top10['stability_score'])):\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{score:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # 4. 综合评估雷达图风格的散点图\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # 散点图：重要性 vs 可信度（用置信区间宽度的倒数表示）\n",
    "    reliability_score = 1 / (most_reliable_top10['ci_width'] + 0.001)  # 避免除零\n",
    "    \n",
    "    scatter = ax4.scatter(most_reliable_top10['combined_importance'], \n",
    "                         reliability_score,\n",
    "                         c=most_reliable_top10['stability_score'],\n",
    "                         s=200, alpha=0.8, cmap='RdYlGn', edgecolors='black')\n",
    "    \n",
    "    # 添加特征名称标注\n",
    "    for i, (_, row) in enumerate(most_reliable_top10.iterrows()):\n",
    "        ax4.annotate(f'特征{i+1}', \n",
    "                    (row['combined_importance'], reliability_score.iloc[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax4.set_xlabel('综合重要性得分', fontsize=12, fontweight='bold')\n",
    "    ax4.set_ylabel('可信度得分 (1/置信区间宽度)', fontsize=12, fontweight='bold')\n",
    "    ax4.set_title('特征重要性 vs 可信度分析(颜色表示稳定性，越绿越稳定)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 添加颜色条\n",
    "    cbar = plt.colorbar(scatter, ax=ax4)\n",
    "    cbar.set_label('稳定性得分', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 输出特征对照表\n",
    "    print(\"\\\\n 特征编号对照表:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, (_, row) in enumerate(most_reliable_top10.iterrows(), 1):\n",
    "        print(f\"特征{i}: {row['feature']}\")\n",
    "    \n",
    "    print(\"\\\\n 最可信Top 10特征统计:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"平均重要性得分: {most_reliable_top10['combined_importance'].mean():.4f}\")\n",
    "    print(f\"平均置信区间宽度: {most_reliable_top10['ci_width'].mean():.4f}\")\n",
    "    print(f\"平均稳定性得分: {most_reliable_top10['stability_score'].mean():.4f}\")\n",
    "    \n",
    "    high_confidence_count = len(most_reliable_top10[most_reliable_top10['ci_width'] <= 0.02])\n",
    "    print(f\"\\\\n高可信度特征数量: {high_confidence_count}/10\")\n",
    "    print(f\"可信度优秀率: {high_confidence_count/10*100:.1f}%\")\n",
    "    \n",
    "    return most_reliable_top10\n",
    "\n",
    "# 执行绘图函数\n",
    "try:\n",
    "    if 'final_importance_df' in locals():\n",
    "        print(\" 开始绘制最可信的Top 10特征图表...\")\n",
    "        \n",
    "        # 绘制图表\n",
    "        top10_reliable = plot_most_reliable_top10_features(final_importance_df)\n",
    "        \n",
    "        print(\"\\\\n 图表绘制完成！\")\n",
    "        \n",
    "    else:\n",
    "        print(\"  请先运行特征重要性分析代码以获得 final_importance_df\")\n",
    "        print(\" 运行顺序：特征重要性分析 → 置信区间分析 → 绘制图表\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"绘图过程中出错: {e}\")\n",
    "    print(\" 请确保已经运行了特征重要性分析的代码\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde520e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义集成预测器类 (必须在加载模型前定义，以支持pickle序列化)\n",
    "# ================================================================\n",
    "\n",
    "class EnsemblePredictor:\n",
    "    \"\"\"\n",
    "    集成预测器类 - 用于加载和使用训练好的多个逻辑回归模型\n",
    "    \"\"\"\n",
    "    def __init__(self, models, feature_names, threshold_strategies):\n",
    "        self.models = models  # 包含model和scaler的列表\n",
    "        self.feature_names = feature_names\n",
    "        self.threshold_strategies = threshold_strategies\n",
    "        self.n_models = len(models)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"预测概率（集成平均）\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X[self.feature_names].values\n",
    "        \n",
    "        all_probas = []\n",
    "        for model_info in self.models:\n",
    "            X_scaled = model_info['scaler'].transform(X)\n",
    "            proba = model_info['model'].predict_proba(X_scaled)[:, 1]\n",
    "            all_probas.append(proba)\n",
    "        \n",
    "        return np.mean(all_probas, axis=0)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"预测类别\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas >= threshold).astype(int)\n",
    "    \n",
    "    def predict_with_strategy(self, X, strategy_name):\n",
    "        \"\"\"使用特定策略预测\"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        \n",
    "        strategy_thresholds = {\n",
    "            '最高召回率策略': self.threshold_strategies[0][1]['threshold'],\n",
    "            '最高F1策略': self.threshold_strategies[1][1]['threshold'],\n",
    "            '最高精确率策略': self.threshold_strategies[2][1]['threshold'],\n",
    "            '业务平衡策略': self.threshold_strategies[3][1]['threshold']\n",
    "        }\n",
    "        \n",
    "        threshold = strategy_thresholds.get(strategy_name, 0.5)\n",
    "        return (probas >= threshold).astype(int), probas\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"获取特征名称\"\"\"\n",
    "        return self.feature_names\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"获取模型信息\"\"\"\n",
    "        return {\n",
    "            'n_models': self.n_models,\n",
    "            'feature_names': self.feature_names,\n",
    "            'available_strategies': ['最高召回率策略', '最高F1策略', '最高精确率策略', '业务平衡策略']\n",
    "        }\n",
    "\n",
    "print(\"✅ EnsemblePredictor 类定义完成，支持pickle序列化\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ada1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"✅ 库导入完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712aa73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 加载训练好的模型\n",
    "# 注意：修改为您实际的模型路径\n",
    "\n",
    "try:\n",
    "    # 请修改为您实际的模型文件路径\n",
    "    model_path = ''\n",
    "    if os.path.exists(model_path):\n",
    "        # 使用joblib加载模型（推荐方式）\n",
    "        trained_model = joblib.load(model_path)\n",
    "        print(f\"✅ 模型加载成功: {model_path}\")\n",
    "        print(f\"📊 模型信息: {trained_model.get_model_info()}\")\n",
    "    else:\n",
    "        print(f\"❌ 模型文件不存在: {model_path}\")\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型加载失败: {e}\")\n",
    "    print(f\"💡 可能原因：\")\n",
    "    print(f\"   1. 模型文件路径不正确\")\n",
    "    print(f\"   2. 需要先运行EnsemblePredictor类定义\")\n",
    "    print(f\"   3. 模型文件版本不兼容\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一部分：数据预处理与特征工程（针对不平衡样本优化）\n",
    "# =================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 模型预测\n",
    "# 预测转化概率\n",
    "try:\n",
    "    # 使用EnsemblePredictor的预测方法\n",
    "    prediction_probabilities = trained_model.predict_proba(X_new)\n",
    "    print(f\"✅ 使用集成模型预测完成\")\n",
    "    print(f\"🎯 模型包含 {trained_model.n_models} 个子模型\")\n",
    "    print(f\"📊 使用特征: {trained_model.get_feature_names()}\")\n",
    "    print(f\"✅ 预测完成，预测了{len(prediction_probabilities)}个用户的转化概率\")\n",
    "    print(f\"📊 概率范围: {prediction_probabilities.min():.3f} - {prediction_probabilities.max():.3f}\")\n",
    "    print(f\"📊 平均概率: {prediction_probabilities.mean():.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 预测失败: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a03806",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_new_data=df\n",
    "# 7. 准备Excel输出数据\n",
    "# 创建详细的用户概率表\n",
    "user_probability_table = pd.DataFrame({\n",
    "    '用户ID': processed_new_data['加密手机号码'],\n",
    "    '转化概率': prediction_probabilities,\n",
    "    '概率百分比': (prediction_probabilities * 100).round(1),\n",
    "})\n",
    "final_output_df = user_probability_table\n",
    "\n",
    "print(f\"✅ 数据整理完成，准备保存Excel文件\")\n",
    "# 将 final_output_df 和 df 通过“加密手机号”连接\n",
    "merged_df = final_output_df.merge(\n",
    "    df, \n",
    "    left_on='用户ID', \n",
    "    right_on='加密手机号码', \n",
    "    how='left'  # or 'inner' if只保留交集\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=[\"用户ID\"], inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef1804",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_excel('全量预测结果f2优化100次.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ef38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280821a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 假设 df 已经存在，且含有 \"转化概率\" 列\n",
    "thresholds = np.arange(0, 1, 0.1)\n",
    "\n",
    "results = {\n",
    "    'threshold': [],\n",
    "    'count': [],\n",
    "    'proportion': []\n",
    "}\n",
    "\n",
    "total = len(df)\n",
    "\n",
    "for t in thresholds:\n",
    "    count = (df['转化概率'] >= t).sum()\n",
    "    proportion = count / total\n",
    "    results['threshold'].append(t)\n",
    "    results['count'].append(count)\n",
    "    results['proportion'].append(proportion)\n",
    "\n",
    "result_df = pd.DataFrame(results)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c7c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
