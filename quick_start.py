#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœºå™¨å­¦ä¹ è½¬åŒ–æ¦‚ç‡é¢„æµ‹ç³»ç»Ÿ - å¿«é€Ÿå¼€å§‹è„šæœ¬
=====================================

è¿™ä¸ªè„šæœ¬æä¾›äº†é¡¹ç›®çš„å¿«é€Ÿå¼€å§‹åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. ç¯å¢ƒæ£€æŸ¥
2. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
3. æ¨¡å‹è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
4. é¢„æµ‹å’Œç»“æœè¾“å‡º

ä½¿ç”¨æ–¹æ³•ï¼š
python quick_start.py --data_path your_data.xlsx --action train
python quick_start.py --data_path your_data.xlsx --action predict --model_path saved_models_xxx/ensemble_predictor.pkl
"""

import argparse
import sys
import os
import pandas as pd
import numpy as np
import warnings
from datetime import datetime

# å¯¼å…¥é…ç½®
try:
    from config import *
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥config.pyï¼Œè¯·ç¡®ä¿config.pyæ–‡ä»¶å­˜åœ¨äºåŒä¸€ç›®å½•")
    sys.exit(1)

warnings.filterwarnings('ignore')

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–åŒ…æ˜¯å¦å·²å®‰è£…"""
    
    print("ğŸ” æ£€æŸ¥ä¾èµ–åŒ…...")
    
    required_packages = [
        'pandas', 'numpy', 'sklearn', 'matplotlib', 
        'seaborn', 'shap', 'joblib', 'openpyxl'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            if package == 'sklearn':
                import sklearn
            else:
                __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"âŒ {package}")
    
    if missing_packages:
        print(f"\nâš ï¸  ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…æ£€æŸ¥é€šè¿‡!")
    return True

def load_and_validate_data(file_path):
    """åŠ è½½å’ŒéªŒè¯æ•°æ®"""
    
    print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    
    try:
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹æ³•
        if file_path.endswith('.parquet'):
            df = pd.read_parquet(file_path)
        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            df = pd.read_excel(file_path)
        elif file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            print("âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨CSVã€Excelæˆ–Parquetæ ¼å¼")
            return None
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—")
        
        # æ£€æŸ¥å¿…è¦åˆ—
        missing_columns = []
        for col in FEATURE_CONFIG['REQUIRED_COLUMNS']:
            if col not in df.columns:
                missing_columns.append(col)
        
        if missing_columns:
            print(f"âš ï¸  ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            print("å¯ç”¨åˆ—:", list(df.columns))
            return None
        
        print("âœ… æ•°æ®æ ¼å¼éªŒè¯é€šè¿‡!")
        return df
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None

def simple_feature_engineering(df):
    """ç®€åŒ–çš„ç‰¹å¾å·¥ç¨‹"""
    
    print("ğŸ”§ æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
    
    from sklearn.preprocessing import LabelEncoder
    
    # åˆ›å»ºå‰¯æœ¬
    data = df.copy()
    
    # åŸºç¡€ç‰¹å¾ç¼–ç 
    le = LabelEncoder()
    categorical_features = ['èµ„æºæ¸ é“']
    
    # å¤„ç†å¯é€‰çš„åˆ†ç±»ç‰¹å¾
    optional_categorical = ['long_å‘¼å«æ—¶æ®µ', 'long_æ˜¯å¦å·¥ä½œæ—¥', 'long_å‘¨å‡ ',
                           'long_å®¢æˆ·æ„å‘_AI', 'long_å®¢æˆ·æ„å‘_äººå·¥']
    
    for col in optional_categorical:
        if col in data.columns:
            categorical_features.append(col)
    
    for col in categorical_features:
        if col in data.columns:
            data[f'{col}_encoded'] = le.fit_transform(data[col].astype(str))
    
    # è¡ç”Ÿç‰¹å¾
    data['æ‰“é€šç‡'] = data['æ‰“é€šæ¬¡æ•°'] / (data['æ‹¨æ‰“æ¬¡æ•°'] + 1e-10)
    data['å®Œæ’­ç‡'] = data['åˆ©ç›Šç‚¹å®Œæ’­æ¬¡æ•°'] / (data['æ‰“é€šæ¬¡æ•°'] + 1e-10)
    
    if 'é€šè¯60sä»¥ä¸Šæ¬¡æ•°' in data.columns:
        data['é•¿é€šè¯ç‡'] = data['é€šè¯60sä»¥ä¸Šæ¬¡æ•°'] / (data['æ‰“é€šæ¬¡æ•°'] + 1e-10)
    
    # é€‰æ‹©ç‰¹å¾
    feature_columns = ['æ‹¨æ‰“æ¬¡æ•°', 'æ‰“é€šæ¬¡æ•°', 'åˆ©ç›Šç‚¹å®Œæ’­æ¬¡æ•°', 'æ‰“é€šç‡', 'å®Œæ’­ç‡']
    
    # æ·»åŠ ç¼–ç åçš„ç‰¹å¾
    for col in categorical_features:
        encoded_col = f'{col}_encoded'
        if encoded_col in data.columns:
            feature_columns.append(encoded_col)
    
    # æ·»åŠ å¯é€‰æ•°å€¼ç‰¹å¾
    optional_numeric = ['é€šè¯15-30sæ¬¡æ•°', 'é€šè¯30-60sæ¬¡æ•°', 'é€šè¯60sä»¥ä¸Šæ¬¡æ•°', 'long_é€šè¯æ—¶é•¿']
    for col in optional_numeric:
        if col in data.columns:
            feature_columns.append(col)
    
    # è¿‡æ»¤å­˜åœ¨çš„ç‰¹å¾
    available_features = [col for col in feature_columns if col in data.columns]
    
    X = data[available_features].fillna(0)
    y = data['æ˜¯å¦è½¬åŒ–æˆäº¤'] if 'æ˜¯å¦è½¬åŒ–æˆäº¤' in data.columns else None
    
    print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(available_features)}ä¸ªç‰¹å¾")
    print(f"   ç‰¹å¾: {available_features}")
    
    return X, y, available_features

def quick_train(X, y, feature_names, n_models=5):
    """å¿«é€Ÿè®­ç»ƒæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    print(f"ğŸ¤– å¼€å§‹å¿«é€Ÿè®­ç»ƒï¼ˆ{n_models}ä¸ªæ¨¡å‹ï¼‰...")
    
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, roc_auc_score
    import joblib
    
    # åˆ†ç¦»æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"è®­ç»ƒé›†: {X_train.shape[0]}æ ·æœ¬, æµ‹è¯•é›†: {X_test.shape[0]}æ ·æœ¬")
    print(f"è®­ç»ƒé›†è½¬åŒ–ç‡: {y_train.mean():.4f}")
    
    # è®­ç»ƒå¤šä¸ªæ¨¡å‹
    models = []
    
    for i in range(n_models):
        print(f"è®­ç»ƒæ¨¡å‹ {i+1}/{n_models}...")
        
        # æ¬ é‡‡æ ·å¤„ç†
        positive_indices = np.where(y_train == 1)[0]
        negative_indices = np.where(y_train == 0)[0]
        
        # éšæœºé€‰æ‹©è´Ÿæ ·æœ¬
        selected_negative = np.random.choice(
            negative_indices, 
            size=len(positive_indices), 
            replace=False
        )
        
        balanced_indices = np.concatenate([positive_indices, selected_negative])
        X_balanced = X_train.iloc[balanced_indices]
        y_balanced = y_train.iloc[balanced_indices]
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_balanced_scaled = scaler.fit_transform(X_balanced)
        
        # è®­ç»ƒæ¨¡å‹
        model = LogisticRegression(
            class_weight='balanced',
            random_state=42,
            max_iter=1000
        )
        model.fit(X_balanced_scaled, y_balanced)
        
        models.append({
            'model': model,
            'scaler': scaler
        })
    
    # é›†æˆé¢„æµ‹
    print("ğŸ” è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    all_probas = []
    for model_info in models:
        X_test_scaled = model_info['scaler'].transform(X_test)
        proba = model_info['model'].predict_proba(X_test_scaled)[:, 1]
        all_probas.append(proba)
    
    ensemble_proba = np.mean(all_probas, axis=0)
    ensemble_pred = (ensemble_proba >= 0.5).astype(int)
    
    # æ€§èƒ½è¯„ä¼°
    print("\nğŸ“Š æ¨¡å‹æ€§èƒ½:")
    print(classification_report(y_test, ensemble_pred))
    
    try:
        auc_score = roc_auc_score(y_test, ensemble_proba)
        print(f"AUCåˆ†æ•°: {auc_score:.4f}")
    except:
        print("æ— æ³•è®¡ç®—AUCï¼ˆå¯èƒ½ç”±äºç±»åˆ«ä¸å¹³è¡¡ï¼‰")
    
    # ä¿å­˜æ¨¡å‹
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_dir = f"quick_trained_models_{timestamp}"
    os.makedirs(model_dir, exist_ok=True)
    
    model_data = {
        'models': models,
        'feature_names': feature_names,
        'timestamp': timestamp,
        'n_models': n_models
    }
    
    model_file = f"{model_dir}/quick_ensemble_model.pkl"
    joblib.dump(model_data, model_file)
    
    print(f"âœ… æ¨¡å‹ä¿å­˜è‡³: {model_file}")
    
    return model_data, model_file

def quick_predict(X, model_path, output_path=None):
    """å¿«é€Ÿé¢„æµ‹"""
    
    print(f"ğŸ”® ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹: {model_path}")
    
    import joblib
    
    # åŠ è½½æ¨¡å‹
    try:
        model_data = joblib.load(model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # é¢„æµ‹
    models = model_data['models']
    feature_names = model_data['feature_names']
    
    # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
    X_pred = X[feature_names]
    
    all_probas = []
    for model_info in models:
        X_scaled = model_info['scaler'].transform(X_pred)
        proba = model_info['model'].predict_proba(X_scaled)[:, 1]
        all_probas.append(proba)
    
    ensemble_proba = np.mean(all_probas, axis=0)
    
    print(f"âœ… é¢„æµ‹å®Œæˆ: {len(ensemble_proba)}ä¸ªæ ·æœ¬")
    print(f"è½¬åŒ–æ¦‚ç‡èŒƒå›´: {ensemble_proba.min():.3f} - {ensemble_proba.max():.3f}")
    print(f"å¹³å‡è½¬åŒ–æ¦‚ç‡: {ensemble_proba.mean():.3f}")
    
    # ä¿å­˜ç»“æœ
    if output_path:
        results_df = pd.DataFrame({
            'è½¬åŒ–æ¦‚ç‡': ensemble_proba,
            'æ¦‚ç‡ç™¾åˆ†æ¯”': (ensemble_proba * 100).round(1)
        })
        
        results_df.to_excel(output_path, index=False)
        print(f"âœ… é¢„æµ‹ç»“æœä¿å­˜è‡³: {output_path}")
    
    return ensemble_proba

def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(description='æœºå™¨å­¦ä¹ è½¬åŒ–æ¦‚ç‡é¢„æµ‹ç³»ç»Ÿ - å¿«é€Ÿå¼€å§‹')
    parser.add_argument('--data_path', type=str, required=True, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--action', type=str, choices=['train', 'predict'], required=True, help='æ‰§è¡ŒåŠ¨ä½œï¼štrain(è®­ç»ƒ) æˆ– predict(é¢„æµ‹)')
    parser.add_argument('--model_path', type=str, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆé¢„æµ‹æ—¶éœ€è¦ï¼‰')
    parser.add_argument('--output_path', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--n_models', type=int, default=5, help='è®­ç»ƒæ¨¡å‹æ•°é‡ï¼ˆé»˜è®¤5ä¸ªï¼‰')
    
    args = parser.parse_args()
    
    print("ğŸš€ æœºå™¨å­¦ä¹ è½¬åŒ–æ¦‚ç‡é¢„æµ‹ç³»ç»Ÿ - å¿«é€Ÿå¼€å§‹")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # æ£€æŸ¥ç¯å¢ƒ
    check_environment()
    
    # åŠ è½½æ•°æ®
    df = load_and_validate_data(args.data_path)
    if df is None:
        sys.exit(1)
    
    # ç‰¹å¾å·¥ç¨‹
    X, y, feature_names = simple_feature_engineering(df)
    
    if args.action == 'train':
        if y is None:
            print("âŒ è®­ç»ƒæ¨¡å¼éœ€è¦ç›®æ ‡å˜é‡'æ˜¯å¦è½¬åŒ–æˆäº¤'")
            sys.exit(1)
        
        print(f"æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {(y==0).sum()}:{(y==1).sum()}")
        
        # è®­ç»ƒæ¨¡å‹
        model_data, model_file = quick_train(X, y, feature_names, args.n_models)
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹æ–‡ä»¶: {model_file}")
        print(f"ä½¿ç”¨å‘½ä»¤è¿›è¡Œé¢„æµ‹: python quick_start.py --data_path æ–°æ•°æ®.xlsx --action predict --model_path {model_file}")
        
    elif args.action == 'predict':
        if not args.model_path:
            print("âŒ é¢„æµ‹æ¨¡å¼éœ€è¦æŒ‡å®š --model_path")
            sys.exit(1)
        
        # é¢„æµ‹
        output_path = args.output_path or f"é¢„æµ‹ç»“æœ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        probabilities = quick_predict(X, args.model_path, output_path)
        
        if probabilities is not None:
            print(f"\nğŸ‰ é¢„æµ‹å®Œæˆï¼")
            print(f"ç»“æœæ–‡ä»¶: {output_path}")
            
            # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
            thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]
            print(f"\næ¦‚ç‡åˆ†å¸ƒ:")
            for threshold in thresholds:
                count = (probabilities >= threshold).sum()
                percentage = count / len(probabilities) * 100
                print(f"  æ¦‚ç‡ â‰¥ {threshold}: {count}ä¸ª ({percentage:.1f}%)")

if __name__ == "__main__":
    main() 